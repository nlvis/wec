{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEC Analysis - Initial Processing\n",
    "\n",
    "*Created 22/03/2019*\n",
    "\n",
    "This notebook contains code that is meant to be run just once, in the beginning of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.parse import CoreNLPParser\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from nltk import pos_tag #alternative POS tagger\n",
    "import pandas as pd\n",
    "#from stanfordcorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map between Treebank and WordNet \n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# WordNet POS tags are: NOUN = 'n', ADJ = 's', VERB = 'v', ADV = 'r', ADJ_SAT = 'a'\n",
    "# Descriptions (c) https://web.stanford.edu/~jurafsky/slp3/10.pdf\n",
    "tag_map = {\n",
    "        'CD':wn.NOUN, # cardinal number (one, two)                           \n",
    "        'EX':wn.ADV, # existential ‘there’ (there)            \n",
    "        'IN':wn.ADV, # preposition/sub-conj (of, in, by)   \n",
    "        'JJ':wn.ADJ,\n",
    "        'JJR':wn.ADJ,\n",
    "        'JJS':wn.ADJ,          \n",
    "        'NN':wn.NOUN, # noun, sing. or mass (llama)          \n",
    "        'NNS':wn.NOUN, # noun, plural (llamas)                  \n",
    "        'NNP':wn.NOUN, # proper noun, sing. (IBM)              \n",
    "        'NNPS':wn.NOUN, # proper noun, plural (Carolinas)\n",
    "        'PDT':wn.ADJ, # predeterminer (all, both)            \n",
    "        'RB':wn.ADV, # adverb (quickly, never)            \n",
    "        'RBR':wn.ADV, # adverb, comparative (faster)        \n",
    "        'RBS':wn.ADV, # adverb, superlative (fastest)     \n",
    "        'RP':wn.ADJ, # particle (up, off)\n",
    "        'VB':wn.VERB, # verb base form (eat)\n",
    "        'VBD':wn.VERB, # verb past tense (ate)\n",
    "#        'VBG':wn.VERB, # verb gerund (eating)\n",
    "        'VBN':wn.VERB, # verb past participle (eaten)\n",
    "        'VBP':wn.VERB, # verb non-3sg pres (eat)\n",
    "        'VBZ':wn.VERB # verb 3sg pres (eats)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "Read Excel spreadsheet into a Pandas dataframe, and transpose the answers into format: Duration, Id, Question, Answer. The answers were previously spellchecked and corrected. We also replaced occurrences of A and B for VarA and VarB, but kept the articles A's in beginning of sentences.\n",
    "\n",
    "The pre-processing includes a POS tagging stage that requires the Stanford CoreNLP (https://stanfordnlp.github.io/CoreNLP/) running in the background. Tagging can alternatively be done using a NLTK parser, for example. For that, instead of pos_tagger.tag below, use pos_tag (uncommenting nltk import pos_tag in the first block).\n",
    "\n",
    "We also specify the POS tags for vara and varb as proper nouns, and also variable and variables as nouns (otherwise they get occasionally tagged as adjectives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\b[\\w-]+\\b',gaps=False)\n",
    "excs = set([])\n",
    "en_stopws = stopwords.words('english') #Load common stopwords\n",
    "#sw_exceptions = set(('X','B'))\n",
    "#stopws = set(en_stopws) - sw_exceptions\n",
    "#excs = set(['increases'])\n",
    "pos_tagger = CoreNLPParser(url='http://localhost:9500', tagtype='pos')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "custom_tags = [('vara','NNP'),('varb','NNP'),('variable','NN'),('variables','NNS') ]\n",
    "Qs = [\"C8P\",\"C6P\",\"C4P\",\"C2P\",\"C0\",\"C2N\",\"C4N\",\"C6N\",\"C8N\"]\n",
    "stop_tags = [\"NN\",\"JJ\",\"NNS\",\"RB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_excel(\"WEC-160.xlsx\")\n",
    "data = pd.melt(raw,id_vars=[\"Duration (in seconds)\",\"ResponseId\"], var_name='Question', value_name='Answer')\n",
    "#data = data.replace(np.nan, '', regex=True)\n",
    "data = data.dropna()\n",
    "if False:\n",
    "    data['Correlation'] = 'n'\n",
    "    data['Relationship'] = 'n'\n",
    "    for idx in data.index:\n",
    "        if ('correlation' in tokenizer.tokenize(data.at[idx,'Answer'])):\n",
    "            data.at[idx,'Correlation'] = 'y' #data[i]['Correlation'] = 'y'     \n",
    "        if ('relationship' in tokenizer.tokenize(data.at[idx,'Answer'])):\n",
    "            data.at[idx,'Relationship'] = 'y' #data[i]['Correlation'] = 'y'\n",
    "    data[(data['Relationship'] == 'n') & (data['Correlation'] == 'n')]\n",
    "#for row in data[(data['Relationship'] == 'n') & (data['Correlation'] == 'n')].itertuples(index=True, name=None):\n",
    "#    print(row[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_tag(original,custom_tags):\n",
    "    idx = original[0] in dict(custom_tags) #custom_tags.index(original[0])\n",
    "    if (idx == False):\n",
    "#        print(original)\n",
    "        return original\n",
    "    else:\n",
    "        pos = dict(custom_tags)[original[0]]\n",
    "#        print(original[0],pos)\n",
    "        return (original[0],pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:  \n",
    "1) Tokenize answers with regular expression that preserves hyphenated words  \n",
    "2) Tag them using Stanford CoreNLP  \n",
    "2.1) Replace the tags for custom words (e.g. VarA and VarB)  \n",
    "3) Apply WordNet lemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokenized'] = data['Answer'].apply(lambda x: tokenizer.tokenize(x))\n",
    "data['tagged'] = data['tokenized'].apply(lambda x: pos_tagger.tag([token.lower() for token in x]))\n",
    "data['tagged'] = data['tagged'].apply(lambda x: [replace_tag((t,p),custom_tags) for t,p in x])\n",
    "#for i,(t,p) in enumerate(snt_tags):\n",
    "#snt_tags[i] = replace_tag((t,p),custom_tags)\n",
    "data['lemmatized'] = data['tagged'].apply(lambda x: [(lemmatizer.lemmatize(word,pos=tag_map[pos]),pos) if pos in tag_map else (word,pos) for word,pos in x])\n",
    "data['stop_tags'] = data['lemmatized'].apply(lambda x: ' '.join([word for word,tag in x if tag in stop_tags]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data to a pickle file\n",
    "data.to_pickle(\"WECdf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenIE -- OPTIONAL\n",
    "\n",
    "Although in the end we did not need to use it, it might be convenient to use information extraction to get relation structures in utterances.\n",
    "This is very slow and not optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(sentence):\n",
    "    nlp = StanfordCoreNLP('http://localhost',9500)\n",
    "    res = nlp.annotate(sentence,\n",
    "                   properties={\n",
    "                       'annotators': 'openie',\n",
    "                       'outputFormat': 'json',\n",
    "                       'timeout': 1000,\n",
    "                   })\n",
    "    res = json.loads(res)\n",
    "    try:\n",
    "        relation = res['sentences'][0]['openie'][0]['relation']\n",
    "        subject = res['sentences'][0]['openie'][0]['subject']\n",
    "        obj = res['sentences'][0]['openie'][0]['object']\n",
    "    except IndexError:\n",
    "        relation = ''\n",
    "        subject = ''\n",
    "        obj = ''\n",
    "    \n",
    "    return (relation,subject,obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['openie'] = data['Answer'].apply(lambda x: get_relation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['openie_replaced'] = data['openie_replaced'].apply(lambda x : none_to_na(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_to_none(t):\n",
    "    if (t[0] == ''):\n",
    "        return (None,None,None)\n",
    "    return t\n",
    "def none_to_na(t):\n",
    "    if (t[0] == None):\n",
    "        return np.nan\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
